{"cells":[{"cell_type":"markdown","source":["## Problem Statement"],"metadata":{"id":"9EaJ8AGwpM-2"}},{"cell_type":"markdown","metadata":{"id":"x3-QehJxbp0t"},"source":["### Business Context\n","\n","Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n","\n","Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n","\n","Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n","\n","The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.).\n","\n","\n","\n","## Objective\n","“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n","\n","The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.\n","The nature of predictions made by the classification model will translate as follows:\n","\n","- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n","- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n","- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n","\n","It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n","\n","“1” in the target variables should be considered as “failure” and “0” represents “No failure”.\n","\n","## Data Description\n","- The data provided is a transformed version of original data which was collected using sensors.\n","- Train.csv - To be used for training and tuning of models.\n","- Test.csv - To be used only for testing the performance of the final best model.\n","- Both the datasets consist of 40 predictor variables and 1 target variable"]},{"cell_type":"markdown","metadata":{"id":"v_-uuGqH-qTt"},"source":["## Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83D17_Wl4jal"},"outputs":[],"source":["# Libraries to help with reading and manipulating data\n","import pandas as pd\n","import numpy as np\n","\n","# Libaries to help with data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# To tune model, get different metric scores, and split data\n","from sklearn.metrics import (\n","    f1_score,\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    confusion_matrix,\n","    roc_auc_score,\n","\n",")\n","from sklearn import metrics\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","\n","# To be used for data scaling and one hot encoding\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n","\n","# To impute missing values\n","from sklearn.impute import SimpleImputer\n","\n","# To oversample and undersample data\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","# To do hyperparameter tuning\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# To be used for creating pipelines and personalizing them\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","# To define maximum number of columns to be displayed in a dataframe\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.max_rows\", None)\n","\n","# To supress scientific notations for a dataframe\n","pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n","\n","# To help with model building\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import (\n","    AdaBoostClassifier,\n","    GradientBoostingClassifier,\n","    RandomForestClassifier,\n","    BaggingClassifier,\n",")\n","from xgboost import XGBClassifier\n","\n","# To suppress scientific notations\n","pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n","\n","# To suppress warnings\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"xxhpZv9y-qTw"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJnKoHy14jam"},"outputs":[],"source":["#Connecting Google drive with Google colab\n","# Reading the data-set into Google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#Reading the  the training set \"Train.csv.csv\" dataset into a dataframe (i.e.loading the train data)\n","path=\"/content/drive/My Drive/Train.csv.csv\"\n","df = pd.read_csv(path)"],"metadata":{"id":"41NnVEoWpdvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reading the  the testing set \"Test.csv.csv\" dataset into a dataframe (i.e.loading the test data)\n","path2=\"/content/drive/My Drive/Test.csv.csv\"\n","df_test = pd.read_csv(path2)"],"metadata":{"id":"SJ7WJ0fxp2ou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a copy of the training dataset\n","data = df.copy()"],"metadata":{"id":"d5JxDDuJsn-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a copy of the test dataset\n","data_test = df_test.copy()"],"metadata":{"id":"SvmAqphOsozF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvpMDcaaMKtI"},"source":["## Data Overview"]},{"cell_type":"markdown","source":["- Observations\n","- Sanity checks\n","-These can be achieved by doing the following;\n","\n","1.   Viewing the first and last few rows of the dataset\n","2.   Checking the shape of the dataset\n","3. Ensuring that the data is stored in the correct format, it's important to identify the data types.\n","4. Getting the statistical summary for the variables.\n","5. Checking for missing values.\n","6. Checking for duplicates\n"],"metadata":{"id":"tIiCRwqZ54_C"}},{"cell_type":"markdown","source":["### Displaying the first few rows of the dataset"],"metadata":{"id":"7v8w7CAqr0hX"}},{"cell_type":"code","source":["# Checking the first five rows of the train data using the dataframe head method\n","data.head()"],"metadata":{"id":"AL-5cDDCsArC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the first five rows of the test data using the dataframe head method\n","data_test.head()"],"metadata":{"id":"ZqpziIG-sODq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checking the shape of the dataset"],"metadata":{"id":"pe8nfj31tpDi"}},{"cell_type":"code","source":["#checking shape of the dataframe to find out the number of rows and columns inthe training data using the dataframe shape command\n","print(\"There are\", data.shape[0], 'rows and', data.shape[1], \"columns.\")"],"metadata":{"id":"U7SZFmPkt0aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking shape of the dataframe to find out the number of rows and columns in the testing data using the dataframe shape command\n","print(\"There are\", data_test.shape[0], 'rows and', data_test.shape[1], \"columns.\")"],"metadata":{"id":"0s0Ew-OLt4AF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checking the columns data types for the dataset"],"metadata":{"id":"WOmBrQjAvsni"}},{"cell_type":"code","source":["# Using the dataframe info() method to print a concise summary of the DataFrame\n","data.info()"],"metadata":{"id":"EBgunmw7vriS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Getting the statistical summary for the variables."],"metadata":{"id":"zj7gmyEhwV8s"}},{"cell_type":"code","source":["# checking the statistical summary of the train data using describe command and transposing it.\n","data.describe().T"],"metadata":{"id":"4gACYl6FwZPg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checking for missing values"],"metadata":{"id":"zgB56tBtw-IX"}},{"cell_type":"code","source":["# Checking for missing values in the training Data\n","data.isnull().sum()"],"metadata":{"id":"dQd7Vm1sxNW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking for missing values in the testing data\n","data_test.isnull().sum()"],"metadata":{"id":"Kue5K2AExQQ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checking for duplicate values"],"metadata":{"id":"PWjU_wqlyZ_1"}},{"cell_type":"code","source":["# checking for duplicate values in the training dataset\n","data.duplicated().sum()"],"metadata":{"id":"jwMzheIuyivt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking for duplicate values in the test dataset\n","data_test.duplicated().sum()"],"metadata":{"id":"Mc0SEMg3Us9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Checking unique values"],"metadata":{"id":"glaaJJ_UysRf"}},{"cell_type":"code","source":["#Checking for unique values in the train dataset\n","data.nunique()"],"metadata":{"id":"ir0sfOa5yvpO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__7ciGcIDPyk"},"source":["## Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","source":["### Univariate analysis"],"metadata":{"id":"zBYA1tIfzfYj"}},{"cell_type":"code","source":["# function to plot a boxplot and a histogram along the same scale.\n","\n","\n","def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n","    \"\"\"\n","    Boxplot and histogram combined\n","\n","    data: dataframe\n","    feature: dataframe column\n","    figsize: size of figure (default (12,7))\n","    kde: whether to the show density curve (default False)\n","    bins: number of bins for histogram (default None)\n","    \"\"\"\n","    f2, (ax_box2, ax_hist2) = plt.subplots(\n","        nrows=2,  # Number of rows of the subplot grid= 2\n","        sharex=True,  # x-axis will be shared among all subplots\n","        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n","        figsize=figsize,\n","    )  # creating the 2 subplots\n","    sns.boxplot(\n","        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n","    )  # boxplot will be created and a triangle will indicate the mean value of the column\n","    sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n","    ) if bins else sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2\n","    )  # For histogram\n","    ax_hist2.axvline(\n","        data[feature].mean(), color=\"green\", linestyle=\"--\"\n","    )  # Add mean to the histogram\n","    ax_hist2.axvline(\n","        data[feature].median(), color=\"black\", linestyle=\"-\"\n","    )  # Add median to the histogram"],"metadata":{"id":"g8846pagzIfs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plotting histograms and boxplots for all the variables"],"metadata":{"id":"jQP7Yu69zaIn"}},{"cell_type":"code","source":["for feature in df.columns:\n","    histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None)"],"metadata":{"id":"q7UBp4r8zYTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking the class distribution in target variable for train data\n","data[\"Target\"].value_counts()"],"metadata":{"id":"K4KSVrcu2zyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking the class distribution in target variable for test data\n","data_test[\"Target\"].value_counts()"],"metadata":{"id":"GWB3l2Lt3GR3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Correlation Check**"],"metadata":{"id":"RvW1sdFraRq-"}},{"cell_type":"code","source":["plt.figure(figsize=(28, 20))\n","sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\n","plt.show()"],"metadata":{"id":"F3tTOut-aOJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knk0w9XH4jao"},"source":["## Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JbJc1bX4jao"},"outputs":[],"source":["# Seperating train data into X and y\n","X = data.drop([\"Target\"], axis=1)\n","y = data[\"Target\"]"]},{"cell_type":"markdown","source":["**The dataset contains both the training and test dataset, therefore the training set will be splitted into two, which will be used as the training and validation set, in order to tune the performance of the validation set**"],"metadata":{"id":"xUBMI02mMdz9"}},{"cell_type":"code","source":["# Splitting train dataset into training and validation set\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)"],"metadata":{"id":"lpVrrZXuM54g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking the shape (rows and columns) of the X_train  dataset\n","print(\"There are\", X_train.shape[0], 'rows and', X_train.shape[1], \"columns.\")"],"metadata":{"id":"9M8-AXTveCsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking the shape (rows and columns) of the X_val  dataset\n","print(\"There are\", X_val.shape[0], 'rows and', X_val.shape[1], \"columns.\")"],"metadata":{"id":"2c8UOoEYeRZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separating test data into X_test and y_test\n","X_test = data_test.drop([\"Target\"], axis = 1)\n","y_test = data_test[\"Target\"]"],"metadata":{"id":"QmOoVAVpfCJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking the shape (rows and columns) of the X_test dataset\n","print(\"There are\", X_test.shape[0], 'rows and', X_test.shape[1], \"columns.\")"],"metadata":{"id":"Gq7jm9wYQHud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0J99-7Kubp09"},"source":["## Missing value imputation\n","\n","\n"]},{"cell_type":"code","source":["# Imputing missing values using the median strategy\n","imputer = SimpleImputer(strategy=\"median\")"],"metadata":{"id":"hke9uYOfBqoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fitting the imputer on train data and transforming the train data\n","X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)"],"metadata":{"id":"Cb5B8HU8RZ6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transforming the validation  data using the imputer fit on train data to avoid data leakage\n","X_val = pd.DataFrame(imputer.fit_transform(X_val), columns=X_train.columns)"],"metadata":{"id":"1i2mUhmsRgjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transforming the test data using the imputer fit on train data to avoid data leakage\n","X_test = pd.DataFrame(imputer.fit_transform(X_test), columns=X_train.columns)"],"metadata":{"id":"sdta6wk8SGBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking that no column has missing values in train sets\n","print(X_train.isna().sum())\n","print(\"-\" * 30)"],"metadata":{"id":"rcwVK984Stof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking that no column has missing values in validation sets\n","print(X_val.isna().sum())\n","print(\"-\" * 30)"],"metadata":{"id":"JHIdcxx9TBs-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking that no column has missing values in test sets\n","print(X_test.isna().sum())\n","print(\"-\" * 30)"],"metadata":{"id":"Zkc2S23TS7nE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Building"],"metadata":{"id":"OzOa9FGA6WtG"}},{"cell_type":"markdown","metadata":{"id":"YZqmoqz7bp0-"},"source":["### Model evaluation criterion"]},{"cell_type":"markdown","source":["The nature of predictions made by the classification model will translate as follows:\n","\n","- True positives (TP) are failures correctly predicted by the model.\n","- False negatives (FN) are real failures in a generator where there is no detection by model.\n","- False positives (FP) are failure detections in a generator where there is no failure.\n","\n","**Which metric to optimize?**\n","\n","* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n","* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n","* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."],"metadata":{"id":"l2ORUgmUjDZC"}},{"cell_type":"markdown","metadata":{"id":"djQTqGKU4jap"},"source":["**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIekBxwp4jaq"},"outputs":[],"source":["# defining a function to compute different metrics to check performance of a classification model built using sklearn\n","def model_performance_classification_sklearn(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check classification model performance\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    acc = accuracy_score(target, pred)  # to compute Accuracy\n","    recall = recall_score(target, pred)  # to compute Recall\n","    precision = precision_score(target, pred)  # to compute Precision\n","    f1 = f1_score(target, pred)  # to compute F1-score\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\n","            \"Accuracy\": acc,\n","            \"Recall\": recall,\n","            \"Precision\": precision,\n","            \"F1\": f1\n","\n","        },\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"markdown","metadata":{"id":"-hOzddAXbp0_"},"source":["### Defining scorer to be used for cross-validation and hyperparameter tuning"]},{"cell_type":"markdown","source":["- We want to reduce false negatives and will try to maximize \"Recall\".\n","- To maximize Recall, we can use Recall as a **scorer** in cross-validation and hyperparameter tuning."],"metadata":{"id":"dzLUm-eSi_cJ"}},{"cell_type":"code","metadata":{"id":"ayLcyOLsbp0_"},"source":["# Type of scoring used to compare parameter combinations\n","scorer = metrics.make_scorer(metrics.recall_score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqCDCbcw4jas"},"source":["### Model Building with original data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-tpzI7g4jas"},"outputs":[],"source":["%%time\n","models = []  # Empty list to store all the models\n","\n","# Appending models into the list\n","models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n","models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n","models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n","models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n","models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n","models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n","models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n","\n","results1 = []  # Empty list to store all model's CV scores\n","names = []  # Empty list to store name of the models\n","\n","\n","# loop through all models to get the mean cross validated score\n","print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n","\n","for name, model in models:\n","    kfold = StratifiedKFold(\n","        n_splits=5, shuffle=True, random_state=1\n","    )  # Setting number of splits equal to 5\n","    cv_result = cross_val_score(\n","        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold\n","    )\n","    results1.append(cv_result)\n","    names.append(name)\n","    print(\"{}: {}\".format(name, cv_result.mean()))\n","\n","print(\"\\n\" \"Validation Performance:\" \"\\n\")\n","\n","for name, model in models:\n","    model.fit(X_train, y_train)\n","    scores = recall_score(y_val, model.predict(X_val))\n","    print(\"{}: {}\".format(name, scores))"]},{"cell_type":"code","source":["# Plotting boxplots for CV scores of all models defined above\n","fig = plt.figure(figsize=(10, 7))\n","\n","fig.suptitle(\"Algorithm Comparison\")\n","ax = fig.add_subplot(111)\n","\n","plt.boxplot(results1)\n","ax.set_xticklabels(names)\n","\n","plt.show()"],"metadata":{"id":"8BI0VlPRT1lk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oBKJaFU24jas"},"source":["### Model Building with Oversampled data\n"]},{"cell_type":"markdown","source":["**Checking counts before an oversampling technique is applied**"],"metadata":{"id":"EHflsTrvUhax"}},{"cell_type":"code","source":["print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n","print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))"],"metadata":{"id":"C6hp6QSWUgCE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Applying over sampling technique using the Synthetic Minority Over Sampling Technique (SMOTE)**"],"metadata":{"id":"MbW_mpabUtbX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKxnygkE4jat"},"outputs":[],"source":["# Synthetic Minority Over Sampling Technique\n","sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n","X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYDlbnUO4jat"},"outputs":[],"source":["print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1)))\n","print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_over == 0)))\n","\n","\n","print(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\n","print(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"]},{"cell_type":"code","source":["%%time\n","models = []  # Empty list to store all the models\n","\n","# Appending models into the list\n","models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n","models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n","models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n","models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n","models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n","models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n","models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n","\n","\n","results1 = []  # Empty list to store all model's CV scores\n","names = []  # Empty list to store name of the models\n","\n","\n","# loop through all models to get the mean cross validated score\n","print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n","\n","for name, model in models:\n","    kfold = StratifiedKFold(\n","        n_splits=5, shuffle=True, random_state=1\n","    )  # Setting number of splits equal to 5\n","    cv_result = cross_val_score(\n","        estimator=model, X=X_train_over, y=y_train_over, scoring = scorer,cv=kfold\n","    )\n","    results1.append(cv_result)\n","    names.append(name)\n","    print(\"{}: {}\".format(name, cv_result.mean()))\n","\n","print(\"\\n\" \"Validation Performance:\" \"\\n\")\n","\n","for name, model in models:\n","    model.fit(X_train_over, y_train_over)\n","    scores = recall_score(y_val, model.predict(X_val))\n","    print(\"{}: {}\".format(name, scores))\n"],"metadata":{"id":"P711uEBvYU08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting boxplots for CV scores of all models defined above\n","fig = plt.figure(figsize=(10, 7))\n","\n","fig.suptitle(\"Algorithm Comparison\")\n","ax = fig.add_subplot(111)\n","\n","plt.boxplot(results1)\n","ax.set_xticklabels(names)\n","\n","plt.show()"],"metadata":{"id":"hdf00u1KYY7l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1aimb6bn4jat"},"source":["### Model Building with Undersampled data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhxfTkvu4jat"},"outputs":[],"source":["# Random undersampler for under sampling the data\n","rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n","X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jROP_DVF4jau"},"outputs":[],"source":["print(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n","print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n","\n","\n","print(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_un == 1)))\n","print(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_un == 0)))\n","\n","\n","print(\"After UnderSampling, the shape of train_X: {}\".format(X_train_un.shape))\n","print(\"After UnderSampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"]},{"cell_type":"code","source":["%%time\n","models = []  # Empty list to store all the models\n","\n","# Appending models into the list\n","models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n","models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n","models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n","models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n","models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n","models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n","models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n","\n","\n","results1 = []  # Empty list to store all model's CV scores\n","names = []  # Empty list to store name of the models\n","\n","\n","# loop through all models to get the mean cross validated score\n","print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n","\n","for name, model in models:\n","    kfold = StratifiedKFold(\n","        n_splits=5, shuffle=True, random_state=1\n","    )  # Setting number of splits equal to 5\n","    cv_result = cross_val_score(\n","        estimator=model, X=X_train_un, y=y_train_un, scoring=scorer, cv=kfold\n","    )\n","    results1.append(cv_result)\n","    names.append(name)\n","    print(\"{}: {}\".format(name, cv_result.mean()))\n","\n","print(\"\\n\" \"Validation Performance:\" \"\\n\")\n","\n","for name, model in models:\n","    model.fit(X_train_un,y_train_un)\n","    scores = recall_score(y_val, model.predict(X_val))\n","    print(\"{}: {}\".format(name, scores))"],"metadata":{"id":"QbDybgolIZzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting boxplots for CV scores of all models defined above\n","fig = plt.figure(figsize=(10, 7))\n","\n","fig.suptitle(\"Algorithm Comparison\")\n","ax = fig.add_subplot(111)\n","\n","plt.boxplot(results1)\n","ax.set_xticklabels(names)\n","\n","plt.show()"],"metadata":{"id":"HwhfthtZL862"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZGY1eL84jau"},"source":["## HyperparameterTuning"]},{"cell_type":"markdown","source":["### Sample Parameter Grids"],"metadata":{"id":"rxM3jQuK_Pqc"}},{"cell_type":"markdown","metadata":{"id":"czq7BZ5b4jau"},"source":["**Hyperparameter tuning can take a long time to run, so to avoid that time complexity - you can use the following grids, wherever required.**\n","\n","- For Gradient Boosting:\n","\n","param_grid = {\n","    \"n_estimators\": np.arange(100,150,25),\n","    \"learning_rate\": [0.2, 0.05, 1],\n","    \"subsample\":[0.5,0.7],\n","    \"max_features\":[0.5,0.7]\n","}\n","\n","- For Adaboost:\n","\n","param_grid = {\n","    \"n_estimators\": [100, 150, 200],\n","    \"learning_rate\": [0.2, 0.05],\n","    \"base_estimator\": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),\n","    ]\n","}\n","\n","- For Bagging Classifier:\n","\n","param_grid = {\n","    'max_samples': [0.8,0.9,1],\n","    'max_features': [0.7,0.8,0.9],\n","    'n_estimators' : [30,50,70],\n","}\n","\n","- For Random Forest:\n","\n","param_grid = {\n","    \"n_estimators\": [200,250,300],\n","    \"min_samples_leaf\": np.arange(1, 4),\n","    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n","    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n","}\n","\n","- For Decision Trees:\n","\n","param_grid = {\n","    'max_depth': np.arange(2,6),\n","    'min_samples_leaf': [1, 4, 7],\n","    'max_leaf_nodes' : [10, 15],\n","    'min_impurity_decrease': [0.0001,0.001]\n","}\n","\n","- For Logistic Regression:\n","\n","param_grid = {'C': np.arange(0.1,1.1,0.1)}\n","\n","- For XGBoost:\n","\n","param_grid={\n","    'n_estimators': [150, 200, 250],\n","    'scale_pos_weight': [5,10],\n","    'learning_rate': [0.1,0.2],\n","    'gamma': [0,3,5],\n","    'subsample': [0.8,0.9]\n","}"]},{"cell_type":"markdown","source":["###Adaboost using oversampled data"],"metadata":{"id":"9mlLOqyASlZj"}},{"cell_type":"code","source":["%%time\n","\n","# defining model\n","Model = AdaBoostClassifier(random_state=1)\n","\n","# Parameter grid to pass in RandomSearchCV\n","param_grid = {\n","    \"n_estimators\": [100, 150, 200],\n","    \"learning_rate\": [0.2, 0.05],\n","    \"base_estimator\": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),\n","    ]\n","}\n","\n","\n","#Calling RandomizedSearchCV\n","randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n","\n","#Fitting parameters in RandomizedSearchCV\n","randomized_cv.fit(X_train_over,y_train_over)\n","\n","print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"],"metadata":{"id":"FLxo-8keSl83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating new pipeline with best parameters\n","tuned_ada = AdaBoostClassifier(\n","    n_estimators= _______, learning_rate= _______, base_estimator= DecisionTreeClassifier(max_depth=_______, random_state=1)\n",") ## Complete the code with the best parameters obtained from tuning\n","\n","tuned_ada.fit(X_train_over, y_train_over)"],"metadata":{"id":"PxRz7E6oS0gL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)\n","ada_train_perf"],"metadata":{"id":"5H2ab-bZYpKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ada_val_perf = model_performance_classification_sklearn(tuned_ada, X_val, y_val)\n","ada_val_perf"],"metadata":{"id":"_FkvuH6aYsWc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tuning Random forest using undersampled data"],"metadata":{"id":"YdS7S3iNZJ-g"}},{"cell_type":"code","source":["%%time\n","\n","# defining model\n","Model = RandomForestClassifier(random_state=1)\n","\n","# Parameter grid to pass in RandomSearchCV\n","param_grid = {\n","    \"n_estimators\": [200,250,300],\n","    \"min_samples_leaf\": np.arange(1, 4),\n","    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n","    \"max_samples\": np.arange(0.4, 0.7, 0.1)}\n","\n","\n","#Calling RandomizedSearchCV\n","randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n","\n","#Fitting parameters in RandomizedSearchCV\n","randomized_cv.fit(X_train_un,y_train_un)\n","\n","print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"],"metadata":{"id":"DhrO_sGWZOHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating new pipeline with best parameters\n","tuned_rf2 = RandomForestClassifier(\n","    max_features='sqrt',\n","    random_state=1,\n","    max_samples=0.5,\n","    n_estimators=300,\n","    min_samples_leaf=2,\n",")\n","\n","tuned_rf2.fit(X_train_un, y_train_un)"],"metadata":{"id":"AQuwhVBPZpFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf2_train_perf = model_performance_classification_sklearn(tuned_rf2, X_train_un, y_train_un)\n","rf2_train_perf"],"metadata":{"id":"Dw3fQOPtaNx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf2_val_perf =model_performance_classification_sklearn(tuned_rf2, X_val, y_val)\n","rf2_val_perf"],"metadata":{"id":"qoEmtkLHaQFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tuning Gradient Boosting using oversampled data"],"metadata":{"id":"tYFt_YMRahJ0"}},{"cell_type":"code","source":["%%time\n","\n","# defining model\n","Model = GradientBoostingClassifier(random_state=1)\n","\n","#Parameter grid to pass in RandomSearchCV\n","param_grid={\"n_estimators\": np.arange(100,150,25), \"learning_rate\": [0.2, 0.05, 1], \"subsample\":[0.5,0.7], \"max_features\":[0.5,0.7]}\n","\n","#Calling RandomizedSearchCV\n","randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, scoring=scorer, n_iter=50, n_jobs = -1, cv=5, random_state=1)\n","\n","#Fitting parameters in RandomizedSearchCV\n","randomized_cv.fit(X_train_over, y_train_over)\n","\n","print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"],"metadata":{"id":"I23UTkrKalGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating new pipeline with best parameters\n","tuned_gbm = GradientBoostingClassifier(\n","    max_features=_______,\n","    random_state=1,\n","    learning_rate=_______,\n","    n_estimators=_______,\n","    subsample=_______,\n",")\n","\n","tuned_gbm.fit(X_train_over, y_train_over)"],"metadata":{"id":"u8IwjUdLarLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbm_train_perf = model_performance_classification_sklearn(tuned_gbm, X_train_over, y_train_over)\n","gbm_train_perf"],"metadata":{"id":"3yMI43lvawPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbm_val_perf =model_performance_classification_sklearn(tuned_gbm, X_val, y_val)\n","gbm_val_perf"],"metadata":{"id":"wcLcNI9ia_Gp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tuning XGBoost using oversampled data"],"metadata":{"id":"xaa3fgnDbXYI"}},{"cell_type":"code","source":["%%time\n","\n","# defining model\n","Model = XGBClassifier(random_state=1,eval_metric='logloss')\n","\n","#Parameter grid to pass in RandomSearchCV\n","param_grid={'n_estimators':[150,200,250],'scale_pos_weight':[5,10], 'learning_rate':[0.1,0.2], 'gamma':[0,3,5], 'subsample':[0.8,0.9]}\n","\n","#Calling RandomizedSearchCV\n","randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n","\n","#Fitting parameters in RandomizedSearchCV\n","randomized_cv.fit(X_train_over, y_train_over)\n","\n","print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"],"metadata":{"id":"9Nm1zuXlbd0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb2 = XGBClassifier(\n","    random_state=1,\n","    eval_metric=\"logloss\",\n","    subsample=_______,\n","    scale_pos_weight=_______,\n","    n_estimators=_______,\n","    learning_rate=_______,\n","    gamma=_______,\n",")\n","\n","xgb2.fit(X_train_over, y_train_over)"],"metadata":{"id":"CRSGqVz2biQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb2_train_perf = model_performance_classification_sklearn(xgb2, X_train_over, y_train_over)\n","xgb2_train_perf"],"metadata":{"id":"NhW17gFmb3Ua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb2_val_perf = model_performance_classification_sklearn(xgb2, X_val, y_val)\n","xgb2_val_perf"],"metadata":{"id":"o8IZEaTgcIrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9JNnpxa4jau"},"source":["## Model performance comparison and choosing the final model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JG85rkY4jav"},"outputs":[],"source":["# training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [\n","        gbm_train_perf.T,\n","        ada_train_perf.T,\n","        rf2_train_perf.T,\n","        xgb2_train_perf.T,\n","    ],\n","    axis=1,\n",")\n","models_train_comp_df.columns = [\n","    \"Gradient Boosting tuned with oversampled data\",\n","    \"AdaBoost classifier tuned with oversampled data\",\n","    \"Random forest tuned with undersampled data\",\n","    \"XGBoost tuned with oversampled data\",\n","]\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","source":["# Validation performance comparison\n","\n","models_val_comp_df = pd.concat(\n","    [\n","        gbm_val_perf.T,\n","        ada_val_perf.T,\n","        rf2_val_perf.T,\n","        xgb2_val_perf.T,\n","    ],\n","    axis=1,\n",")\n","models_val_comp_df.columns = [\n","    \"Gradient Boosting tuned with oversampled data\",\n","    \"AdaBoost classifier tuned with oversampled data\",\n","    \"Random forest tuned with undersampled data\",\n","    \"XGBoost tuned with oversampled data\",\n","]\n","print(\"Validation performance comparison:\")\n","models_val_comp_df"],"metadata":{"id":"Ff6fg8uTcqXK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_pDMFAz4jav"},"source":["### Test set final performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8-epsXv4jav"},"outputs":[],"source":["# Calculating different metrics on the test set\n","gbm_grid_test = model_performance_classification_sklearn(, X_test, y_test)\n","print(\"Test performance:\")\n","gbm_grid_test"]},{"cell_type":"markdown","source":["### Feature Importances"],"metadata":{"id":"oUG0JXU7jQ0J"}},{"cell_type":"code","source":["feature_names = X_train.columns\n","importances =  '_______' ## Complete the code to check the feature importance of the best model\n","indices = np.argsort(importances)\n","\n","plt.figure(figsize=(12, 12))\n","plt.title(\"Feature Importances\")\n","plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n","plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","plt.xlabel(\"Relative Importance\")\n","plt.show()"],"metadata":{"id":"kr1ZdcKMjUoN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TM6VZTRn4jav"},"source":["## Pipelines to build the final model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zzg12gvx4jav"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"c5hPmHyR4jaw"},"source":["# Business Insights and Conclusions"]},{"cell_type":"markdown","source":["***"],"metadata":{"id":"VB3eO21n_sgt"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["TM6VZTRn4jav","c5hPmHyR4jaw"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}